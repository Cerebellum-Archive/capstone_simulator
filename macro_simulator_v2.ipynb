{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To Do\n",
    "# xgboost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Author = 'Conrad'\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pytz\n",
    "local_timezone = 'America/Los_Angeles'\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.plotting import scatter_matrix\n",
    "import xarray as xr\n",
    "from pandas import Series\n",
    "import seaborn as sns\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "from sklearn.preprocessing import Binarizer as binarizer\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import cluster\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import mixture\n",
    "                                  \n",
    "from sklearn.metrics import mean_squared_error as rmse \n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import explained_variance_score as evs\n",
    "from sklearn.metrics import r2_score as r_squared\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "                                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/29/23 08:39:28\n"
     ]
    }
   ],
   "source": [
    "# SET KEYLINE TIME RELATED PARAMETERS\n",
    "calendar = {}\n",
    "calendar['z_score_window'] = 252\n",
    "calendar['regression_est_window'] = 400\n",
    "calendar['sim_time'] = time.strftime('%x %X')\n",
    "calendar['trim_pct'] = .01\n",
    "calendar['ewa_halflife'] = 3\n",
    "\n",
    "print(calendar['sim_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR TIME-SAFE Z-SCORES\n",
    "def z_score(df,window = calendar['z_score_window']):\n",
    "    window = calendar['z_score_window']\n",
    "    rstd = df.rolling(window=window,min_periods=window).std()\n",
    "    rmean = df.rolling(window=window,min_periods=window).mean()\n",
    "    z = (df - rmean) / rstd\n",
    "    z = z.dropna()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Re-Basing Returns & computing log returns from a price levels\n",
    "\n",
    "def rebase_rets(input):\n",
    "    output = pd.DataFrame(input).set_index(pd.DatetimeIndex(input.index))\n",
    "    output = output.dropna()\n",
    "    output = 1+output.cumsum()\n",
    "    return output\n",
    "\n",
    "def log_returns(df):\n",
    "    df = np.log(df)\n",
    "    df = df - df.shift(1)\n",
    "    #df.index.name = None\n",
    "    return df\n",
    "\n",
    "def pct_returns(df):\n",
    "    df = df.pct_change()\n",
    "    return df\n",
    "\n",
    "def generate_train_predict_calender(df, window_size):\n",
    "    date_ranges = []\n",
    "    index = df.index\n",
    "    num_days = len(index)\n",
    "\n",
    "    for i in range(0, num_days - window_size):\n",
    "        #if i + window_size <= len(index):\n",
    "        train_start_date = index[i]\n",
    "        train_end_date = index[i + window_size - 1]\n",
    "        prediction_date = index[i + window_size]\n",
    "        date_ranges.append([train_start_date, train_end_date, prediction_date])\n",
    "\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "def apply_ewm(X, halflife):\n",
    "    return X.ewm(halflife=halflife).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_func_1(df,pred_col='predicted', actual_col='actual',new_col='perf'):\n",
    "    t_conditions = [ df[pred_col] >= 0, df[pred_col] < 0 ]\n",
    "    t_positions =  [ df[actual_col]*1,  df[actual_col]*-1 ]\n",
    "    return np.select(t_conditions,t_positions, default=np.nan) # Apply trading logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  10 of 10 completed\n"
     ]
    }
   ],
   "source": [
    "# Synch Indexes & Preprocess Features and Targets\n",
    "spdr = ['SPY','XLK', 'XLF', 'XLV', 'XLY', 'XLP', 'XLE', 'XLI', 'XLB', 'XLU'] # XLRE????\n",
    "#df_all = yf.download(spdr, start=\"1990-01-03\", end=\"2024-08-06\")['Adj Close'].dropna()\n",
    "df_all = yf.download(spdr, start=\"2000-01-03\", end=\"2016-08-05\")['Adj Close'].dropna()\n",
    "\n",
    "y_level = df_all['SPY']  # break out targets\n",
    "y = log_returns(y_level)  # process as log returns\n",
    "\n",
    "X_level = df_all.drop(['SPY'],axis=1)   # break out features\n",
    "X_log_returns = log_returns(X_level).shift(1)  # Lagging all features 1 day \n",
    "X = z_score(X_log_returns,calendar['z_score_window']) # preprocess as z-scores\n",
    "\n",
    "y = y.loc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2001-01-03    0.046917\n",
       "2001-01-04   -0.010823\n",
       "2001-01-05   -0.033187\n",
       "2001-01-08    0.007711\n",
       "2001-01-09   -0.002644\n",
       "                ...   \n",
       "2016-07-29    0.001613\n",
       "2016-08-01   -0.000829\n",
       "2016-08-02   -0.006428\n",
       "2016-08-03    0.002918\n",
       "2016-08-04    0.001063\n",
       "Name: SPY, Length: 3921, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "halflife = 3\n",
    "\n",
    "# Create a pipeline with FunctionTransformer and RandomForestRegressor\n",
    "pipe = Pipeline([('ewm', FunctionTransformer(apply_ewm, kw_args={'halflife': halflife}))])\n",
    "#pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sim_stats(reg_out,testlabel,author='CG'):\n",
    "    df = pd.DataFrame()\n",
    "    df.loc['mean',testlabel] = mean = 252*reg_out.perf.mean()\n",
    "    df.loc['stdev',testlabel] = std = (np.sqrt(252))*reg_out.perf.std()\n",
    "    df.loc['sharpe',testlabel] = mean / std\n",
    "    df.loc['rmse',testlabel] = np.sqrt(rmse(reg_out.predicted,reg_out.actual))\n",
    "    df.loc['mae',testlabel] = mae(reg_out.predicted,reg_out.actual)\n",
    "    df.loc['evs',testlabel] = evs(reg_out.predicted,reg_out.actual)\n",
    "    df.loc['r2',testlabel] = r_squared(reg_out.predicted,reg_out.actual)\n",
    "    df.loc['benchmark return',testlabel] = bench_ret = 252*reg_out.actual.mean()\n",
    "    df.loc['benchmark std',testlabel] = bench_std = (np.sqrt(252))*reg_out.actual.std()\n",
    "    df.loc['benchmark sharpe',testlabel] = bench_ret / bench_std\n",
    "    df.loc['beg_pred',testlabel] = min(reg_out.actual.index).date()\n",
    "    df.loc['end_pred',testlabel] = max(reg_out.actual.index).date()\n",
    "    df.loc['train window',testlabel] = window=calendar['regression_est_window'] \n",
    "    df.loc['z_score_window',testlabel] = calendar['z_score_window']\n",
    "    #df.loc['sim_time @ author',testlabel] =  datetime.now(pytz.timezone(local_timezone)).strftime(\"%Y-%m-%d %H:%M\")\n",
    "    df.loc['sim_time',testlabel] =  datetime.now(pytz.timezone(local_timezone)).strftime(\"%x %-I:%-m%p\") \n",
    "    df.loc['author',testlabel] = author\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROLL FORWARD SIMULATOR - 1) TRAIN UP TO T-1, 2) PREDICT AT T, 3) TRADE\n",
    "\n",
    "def Simulate(X,y,window=calendar['regression_est_window'],pipe_steps={},dims_dict={},tfunc=t_func_1):    \n",
    "    regout = pd.DataFrame(index=y.index)\n",
    "    \n",
    "    date_ranges = generate_train_predict_calender(X, window)\n",
    "    \n",
    "    pipe = Pipeline(steps=pipe_steps)\n",
    "    pipe.set_params(**dims_dict)\n",
    "\n",
    "    for n,dates in enumerate(date_ranges):\n",
    "\n",
    "        start_training, end_training, prediction_date = dates[0], dates[1],dates[2]\n",
    "\n",
    "        fit_X = X[start_training:end_training]\n",
    "        fit_y = y[start_training:end_training]\n",
    "        pred_X = X[prediction_date:prediction_date]\n",
    "        \n",
    "        pipe.fit(fit_X,fit_y)\n",
    "        #print('pred_date',prediction_date,'features = ',pipe.named_steps['eln'].n_features_in_)\n",
    "        #regout.loc[prediction_date,'expl_var'] = pipe.named_steps['pca'].explained_variance_ratio_[2]\n",
    "        #regout.loc[prediction_date,'coeff_1st_dim'] = pipe.named_steps['reg'].coef_[0]\n",
    "                \n",
    "        regout.loc[prediction_date,'predicted'] = pipe.predict(pred_X)\n",
    "            \n",
    "    # Add actual spy returns\n",
    "    regout['actual'] = y[regout.predicted.index].dropna()\n",
    "    regout.loc[regout['predicted'].dropna().index,'rand_pred'] = np.random.permutation(regout['predicted'].dropna())\n",
    "    \n",
    "    # Add trading rule and store performance in 'perf' and 'mtm' columns\n",
    "    regout = regout.dropna()\n",
    "    regout['perf'] = tfunc(regout,pred_col='predicted', actual_col='actual',new_col = 'perf')   \n",
    "    #regout['mtm'] = (1+regout['perf']).cumprod()-1  # for arithmatic returns\n",
    "    regout['mtm'] = regout['perf'].cumsum()   # for log returns\n",
    "    \n",
    "    regout['rand_perf'] = tfunc(regout,pred_col='rand_pred', actual_col='actual',new_col='rand_perf')  \n",
    "    #regout['rand_mtm'] = (1+regout['rand_perf']).cumprod()-1\n",
    "    regout['rand_mtm'] = regout['rand_perf'].cumsum()   # for log returns\n",
    "    \n",
    "    return regout.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca1reg sim time =  0:00:03.063498\n",
      "pca2reg sim time =  0:00:03.050243\n",
      "pca3reg sim time =  0:00:03.110579\n",
      "pca4reg sim time =  0:00:03.099710\n",
      "pca5reg sim time =  0:00:03.160025\n",
      "pca6reg sim time =  0:00:03.176495\n",
      "pca7reg sim time =  0:00:03.131677\n",
      "pca8reg sim time =  0:00:03.141874\n",
      "pca9reg sim time =  0:00:03.126591\n"
     ]
    }
   ],
   "source": [
    "# Simulate Dim Reduction \n",
    "last_printed_time = datetime.now()\n",
    "\n",
    "## Dim Reduction\n",
    "pca = PCA()\n",
    "clust = cluster.FeatureAgglomeration(n_clusters=4)\n",
    "kbest = SelectKBest(chi2, k=20)\n",
    "\n",
    "## Estimators\n",
    "ols = LinearRegression()\n",
    "eln = ElasticNet()\n",
    "lasso = Lasso(alpha=0.1)\n",
    "ridge = Ridge(alpha=.5)\n",
    "\n",
    "Results_xr = xr.Dataset()\n",
    "Results_df = pd.DataFrame()\n",
    "\n",
    "max_sweep_n = 9\n",
    "sweep_n = [n+1 for n in range(max_sweep_n)]\n",
    "pca_tags = sweep_tags = ['pca'+str(n)+'reg' for n in sweep_n]\n",
    "\n",
    "for n, test_param in enumerate(sweep_n):\n",
    "    pipe_steps=[('pca', PCA()), ('ols', LinearRegression())]\n",
    "    dims_dict = {'pca__n_components':test_param,'ols__fit_intercept':True}\n",
    "    regout_df = Simulate(X,y,window=calendar['regression_est_window'],pipe_steps=pipe_steps,dims_dict=dims_dict,tfunc=t_func_1)\n",
    "    \n",
    "    Results_df = pd.concat([Results_df,sim_stats(regout_df,sweep_tags[n])],axis=1)\n",
    "    sim_xr = regout_df.to_xarray().expand_dims(Tag=[sweep_tags[n]])    \n",
    "    Results_xr = xr.merge([Results_xr,sim_xr])\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    time_elapsed = current_time - last_printed_time\n",
    "    last_printed_time = current_time\n",
    "    print(sweep_tags[n],'sim time = ',time_elapsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1reg</th>\n",
       "      <th>pca2reg</th>\n",
       "      <th>pca3reg</th>\n",
       "      <th>pca4reg</th>\n",
       "      <th>pca5reg</th>\n",
       "      <th>pca6reg</th>\n",
       "      <th>pca7reg</th>\n",
       "      <th>pca8reg</th>\n",
       "      <th>pca9reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.097098</td>\n",
       "      <td>0.113633</td>\n",
       "      <td>0.07113</td>\n",
       "      <td>0.056302</td>\n",
       "      <td>0.070145</td>\n",
       "      <td>0.080684</td>\n",
       "      <td>0.07361</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.059777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stdev</th>\n",
       "      <td>0.193095</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>0.193139</td>\n",
       "      <td>0.193159</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>0.193125</td>\n",
       "      <td>0.193136</td>\n",
       "      <td>0.193191</td>\n",
       "      <td>0.193155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpe</th>\n",
       "      <td>0.502854</td>\n",
       "      <td>0.588593</td>\n",
       "      <td>0.368282</td>\n",
       "      <td>0.291483</td>\n",
       "      <td>0.36318</td>\n",
       "      <td>0.41778</td>\n",
       "      <td>0.381129</td>\n",
       "      <td>0.038639</td>\n",
       "      <td>0.309476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.012189</td>\n",
       "      <td>0.012212</td>\n",
       "      <td>0.012204</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.012276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.008109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evs</th>\n",
       "      <td>-127.953297</td>\n",
       "      <td>-60.662885</td>\n",
       "      <td>-54.257627</td>\n",
       "      <td>-47.455467</td>\n",
       "      <td>-38.531824</td>\n",
       "      <td>-35.293854</td>\n",
       "      <td>-31.479385</td>\n",
       "      <td>-29.294251</td>\n",
       "      <td>-27.023484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-127.959541</td>\n",
       "      <td>-60.665517</td>\n",
       "      <td>-54.260075</td>\n",
       "      <td>-47.457517</td>\n",
       "      <td>-38.533865</td>\n",
       "      <td>-35.295488</td>\n",
       "      <td>-31.481052</td>\n",
       "      <td>-29.295919</td>\n",
       "      <td>-27.024883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark return</th>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.081637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark std</th>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.193123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark sharpe</th>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "      <td>0.42272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beg_pred</th>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "      <td>2002-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_pred</th>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train window</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_score_window</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim_time</th>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "      <td>11/29/23 8:11AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "      <td>CG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pca1reg          pca2reg          pca3reg  \\\n",
       "mean                     0.097098         0.113633          0.07113   \n",
       "stdev                    0.193095         0.193059         0.193139   \n",
       "sharpe                   0.502854         0.588593         0.368282   \n",
       "rmse                     0.012186         0.012161         0.012189   \n",
       "mae                      0.007983         0.007981         0.007999   \n",
       "evs                   -127.953297       -60.662885       -54.257627   \n",
       "r2                    -127.959541       -60.665517       -54.260075   \n",
       "benchmark return         0.081637         0.081637         0.081637   \n",
       "benchmark std            0.193123         0.193123         0.193123   \n",
       "benchmark sharpe          0.42272          0.42272          0.42272   \n",
       "beg_pred               2002-08-12       2002-08-12       2002-08-12   \n",
       "end_pred               2016-08-04       2016-08-04       2016-08-04   \n",
       "train window                  400              400              400   \n",
       "z_score_window                252              252              252   \n",
       "sim_time          11/29/23 8:11AM  11/29/23 8:11AM  11/29/23 8:11AM   \n",
       "author                         CG               CG               CG   \n",
       "\n",
       "                          pca4reg          pca5reg          pca6reg  \\\n",
       "mean                     0.056302         0.070145         0.080684   \n",
       "stdev                    0.193159         0.193141         0.193125   \n",
       "sharpe                   0.291483          0.36318          0.41778   \n",
       "rmse                     0.012212         0.012204         0.012236   \n",
       "mae                      0.008023         0.008043         0.008063   \n",
       "evs                    -47.455467       -38.531824       -35.293854   \n",
       "r2                     -47.457517       -38.533865       -35.295488   \n",
       "benchmark return         0.081637         0.081637         0.081637   \n",
       "benchmark std            0.193123         0.193123         0.193123   \n",
       "benchmark sharpe          0.42272          0.42272          0.42272   \n",
       "beg_pred               2002-08-12       2002-08-12       2002-08-12   \n",
       "end_pred               2016-08-04       2016-08-04       2016-08-04   \n",
       "train window                  400              400              400   \n",
       "z_score_window                252              252              252   \n",
       "sim_time          11/29/23 8:11AM  11/29/23 8:11AM  11/29/23 8:11AM   \n",
       "author                         CG               CG               CG   \n",
       "\n",
       "                          pca7reg          pca8reg          pca9reg  \n",
       "mean                      0.07361         0.007465         0.059777  \n",
       "stdev                    0.193136         0.193191         0.193155  \n",
       "sharpe                   0.381129         0.038639         0.309476  \n",
       "rmse                     0.012253         0.012276         0.012276  \n",
       "mae                      0.008082         0.008107         0.008109  \n",
       "evs                    -31.479385       -29.294251       -27.023484  \n",
       "r2                     -31.481052       -29.295919       -27.024883  \n",
       "benchmark return         0.081637         0.081637         0.081637  \n",
       "benchmark std            0.193123         0.193123         0.193123  \n",
       "benchmark sharpe          0.42272          0.42272          0.42272  \n",
       "beg_pred               2002-08-12       2002-08-12       2002-08-12  \n",
       "end_pred               2016-08-04       2016-08-04       2016-08-04  \n",
       "train window                  400              400              400  \n",
       "z_score_window                252              252              252  \n",
       "sim_time          11/29/23 8:11AM  11/29/23 8:11AM  11/29/23 8:11AM  \n",
       "author                         CG               CG               CG  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Results_xr#.rand_perf.mean(dim='Date').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice results by in and out of sample\n",
    "test_df = pd.DataFrame()\n",
    "IS = slice(\"2001-07-26\",\"2016-08-04\")  #  In-Sample \n",
    "OOS = slice(\"2016-08-05\", \"2023-11-27\")  #  Out of Sample\n",
    "def xr_2_df_by_slice(tag, drange, label):\n",
    "    xr_out = Results_xr.sel(Tag=[tag],Date=drange).to_dataframe()\n",
    "    xr_out.reset_index(level='Tag', inplace=True)\n",
    "    return sim_stats(xr_out,label,author='CG')\n",
    "\n",
    "tag = 'pca2reg'\n",
    "oos_df = xr_2_df_by_slice(tag, OOS,'OOS_'+tag)\n",
    "is_df = xr_2_df_by_slice(tag, IS,'IS_'+tag)\n",
    "is_oos_df = pd.concat([oos_df, is_df],axis=1)\n",
    "is_oos_df\n",
    "\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_xr.sel(Date=IS)\n",
    "#Results_xr.sel(Date=OOS)\n",
    "#Results_xr.sel(Date=OOS).mean(dim='Date').mean(dim='Tag')\n",
    "#Results_xr.sel(Date=OOS).mean(dim='Date').mean(dim='Tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tag in sweep_tags:\n",
    "\n",
    "\n",
    "xr_2_df_by_slice(Tag = 'pca3reg', Date=OOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Results_xr['mtm'].plot.line(x='Date')\n",
    "Results_xr['mtm'].sel(Date=slice(\"2016-08-06\", \"2023-11-27\")).cumsum.plot()#.line(x='Date')\n",
    "plt.title('K-Best with F-Test then Linreg with N Features passed')\n",
    "plt.ylabel('Cummulative Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
    "kbest = SelectKBest(chi2, k=20)\n",
    "\n",
    "max_sweep_n = 2\n",
    "sweep_n = [n+1 for n in range(max_sweep_n)]\n",
    "kbest_tags = sweep_tags = ['kbest'+str(n)+'reg' for n in sweep_n]\n",
    "\n",
    "Results_kbest_xr = xr.Dataset()\n",
    "Summary_list = []\n",
    "\n",
    "for n, test_param in enumerate(sweep_n):\n",
    "    pipe_steps=[('kb', SelectKBest(k=test_param,score_func=f_regression)), ('ols', LinearRegression(fit_intercept=True))]\n",
    "    dims_dict = {}\n",
    "\n",
    "    regout_df = Simulate(X,y,window=calendar['regression_est_window'],pipe_steps=pipe_steps,dims_dict=dims_dict)\n",
    "    Results_df = pd.concat([Results_df,sim_stats(regout_df,sweep_tags[n])],axis=1)\n",
    "    sim_xr = regout_df.to_xarray().expand_dims(Tag=[sweep_tags[n]])    \n",
    "    Results_xr = xr.merge([Results_xr,sim_xr])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Results_xr['mtm'].sel(Tag=pca_tags).plot.line(x='Date')\n",
    "plt.title('K-Best with F-Test then Linreg with N Features passed')\n",
    "plt.ylabel('Cummulative Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Results_df.T['mean'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimators = [\n",
    "#     (\"OLS\", LinearRegression()),\n",
    "#     (\"Theil-Sen\", TheilSenRegressor(random_state=42)),\n",
    "#     (\"RANSAC\", RANSACRegressor(random_state=42)),\n",
    "# ]\n",
    "\n",
    "ols =  LinearRegression(fit_intercept=True)\n",
    "ridge = Ridge(alpha=.9,fit_intercept=True)\n",
    "eln = ElasticNet(random_state=0,alpha=.5,l1_ratio=.5,fit_intercept=True,selection='random')\n",
    "\n",
    "learners = [ols, ridge, eln]\n",
    "learner_tags = sweep_tags = [str(x) for x in learners]\n",
    "\n",
    "\n",
    "for n, learner_tested in enumerate(learners):\n",
    "    pipe_steps=[('pca', PCA(n_components=2)), (learner_tags[n], learners[n])]\n",
    "    dims_dict = {}\n",
    "\n",
    "    regout_df = Simulate(X,y,window=calendar['regression_est_window'],pipe_steps=pipe_steps,dims_dict=dims_dict)\n",
    "    Results_df = pd.concat([Results_df,sim_stats(regout_df,sweep_tags[n])],axis=1)\n",
    "    sim_xr = regout_df.to_xarray().expand_dims(Tag=[sweep_tags[n]])    \n",
    "    Results_xr = xr.merge([Results_xr,sim_xr])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Results_xr['mtm'].sel(Tag=learner_tags).plot.line(x='Date')\n",
    "plt.title('K-Best with F-Test then Linreg with N Features passed')\n",
    "plt.ylabel('Cummulative Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('ewm', FunctionTransformer(apply_ewm, kw_args={'halflife': halflife}))])\n",
    "\n",
    "\n",
    "max_sweep_n = 5\n",
    "sweep_n = [n+1 for n in range(1,max_sweep_n)]\n",
    "kbest_tags = sweep_tags = ['ewm_'+str(n)+'_reg' for n in sweep_n]\n",
    "\n",
    "#Results_kbest_xr = xr.Dataset()\n",
    "#Summary_list = []\n",
    "\n",
    "for n, test_param in enumerate(sweep_n):\n",
    "    pipe_steps=[('ewm', FunctionTransformer(apply_ewm, kw_args={'halflife': test_param})), ('ols', LinearRegression(fit_intercept=True))]\n",
    "    dims_dict = {}\n",
    "\n",
    "    regout_df = Simulate(X,y,window=calendar['regression_est_window'],pipe_steps=pipe_steps,dims_dict=dims_dict)\n",
    "    Results_df = pd.concat([Results_df,sim_stats(regout_df,sweep_tags[n])],axis=1)\n",
    "    sim_xr = regout_df.to_xarray().expand_dims(Tag=[sweep_tags[n]])    \n",
    "    Results_xr = xr.merge([Results_xr,sim_xr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_xr['mtm'].sel(Tag=kbest_tags).plot.line(x='Date')\n",
    "plt.title('K-Best with F-Test then Linreg with N Features passed')\n",
    "plt.ylabel('Cummulative Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate random guess bets with 50% up and 50% down\n",
    "# Returns a,b : a = regression metrics and b = df of out of sample predictions and targets\n",
    "def random_bets(max_trials=50):\n",
    "    rand_results=pd.DataFrame()\n",
    "    rand_grets=pd.DataFrame()\n",
    "    for t in range(max_trials):\n",
    "        tags = 'Ramdom ' + str(t)\n",
    "        X_bench = pd.DataFrame(np.random.choice([1,-1],size=(len(y.index),1)),columns=['Random'],index=y.index)\n",
    "        rand_out = rreg(X_bench,y,drop_pred_indercept = False)\n",
    "        rand_trade_out, rand_oos  = trule(rand_out)\n",
    "        col_rand_results = regmetrics(rand_out,rand_trade_out,'tags')\n",
    "        \n",
    "        rand_results[tags] = col_rand_results['tags']\n",
    "        rand_grets[tags] = rand_oos['strat_ret']\n",
    "    return rand_results, rand_grets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATA ANALYSIS - VIEW CORRELATION PROPERTIES BETWEEN SPY AND EACH OF ITS SECTOR COMPONENTS\n",
    "import seaborn as sns\n",
    "corr = dl.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "ax = sns.heatmap(corr, mask=mask, square=True, annot=True, cmap='RdBu')\n",
    "ax.get_figure().savefig('correl.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, cluster\n",
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "X = np.reshape(images, (len(images), -1))\n",
    "agglo = cluster.FeatureAgglomeration(n_clusters=32)\n",
    "agglo.fit(X)\n",
    "X_reduced = agglo.transform(X)\n",
    "print(X_reduced.shape,X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  this is the end of testing - the rest are summary / diagnostics, some of which run very slowly\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# COMPUTE N RANDOM STRATEGIES (50% up 50% down guess) AND GRAPH PERFOMANCE\n",
    "a , Rand_Rets = random_bets(100)\n",
    "c = rebase_rets(Rand_Rets)\n",
    "print(\"Average Return when guessing = \", a.T['return'].mean())\n",
    "c.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Classify the results into Up vs Down and then compute Confusion Matrix and Classification Report\n",
    "#binarizer.__init__(threshold=0.0,copy=True)  # get an instance of the class\n",
    "binarizer = preprocessing.Binarizer().fit(X)\n",
    "Xbin = pd.DataFrame(binarizer.fit_transform(gretsX),index=gretsX.index)\n",
    "ybin = pd.DataFrame(y,index=gretsX.index)\n",
    "ybin = pd.DataFrame(binarizer.transform(ybin),index=ybin.index)\n",
    "class_names = ['up','down']\n",
    "for col in Xbin.columns:\n",
    "    print(col)\n",
    "    print(classification_report(ybin,Xbin[col],target_names=class_names))\n",
    "    #print(col)\n",
    "    #test = Xbin[col]\n",
    "    #print(test.tail)\n",
    "    cfm = confusion_matrix(ybin,Xbin[col])\n",
    "    print(cfm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cfm, classes=class_names,\n",
    "                      title='Confusion matrix, Need Title')\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Classify the Random results into Up vs Down and then compute Confusion Matrix and Classification Report\n",
    "a , random_rets = random_bets(5)\n",
    "\n",
    "Xrandbin = pd.DataFrame(binarizer.transform(random_rets),index=random_rets.index)\n",
    "ybin = pd.DataFrame(y,index=random_rets.index)\n",
    "ybin = pd.DataFrame(binarizer.transform(ybin),index=ybin.index)\n",
    "for col in Xrandbin.columns:\n",
    "    print(classification_report(ybin,Xrandbin[col],target_names=class_names))\n",
    "    cfm = confusion_matrix(ybin,Xrandbin[col])\n",
    "    print(cfm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cfm, classes=class_names,\n",
    "                      title='Confusion matrix, Need Title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#RESULTS - GRAPH RECOMMENDED MODEL AGAINST NON-PCA BENCHMARK, SPY, AND NON-SCORED FEATURES\n",
    "final_graph = rebase_rets(pd.concat([gretsX['DL_PCA 2'], gretsXz['DLZ_PCA 2'],Regg_Rets['Non-PCA_Benchmark'],spy], axis=1, join='inner').dropna())\n",
    "final_graph.plot()\n",
    "figg = plt.gcf()\n",
    "figg.savefig('final_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_table = pd.concat([resultsX['DL_PCA 2'], resultsXz['DLZ_PCA 2']], axis=1, join='inner').dropna()\n",
    "\n",
    "final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import *\n",
    "\n",
    "# Get all names in the module\n",
    "all_names = dir(linear_model)\n",
    "\n",
    "# Filter out classes from the names\n",
    "#model_names = [name for name in all_names if isinstance(getattr(linear_model, name), type)]\n",
    "\n",
    "model_names = [getattr(linear_model, name) for name in all_names if\n",
    "                     isinstance(getattr(linear_model, name), type) and\n",
    "                     issubclass(getattr(linear_model, name), RegressorMixin)]\n",
    "\n",
    "# Print the list of model names\n",
    "print(model_names)\n",
    "\n",
    "sweep_results = pd.DataFrame(columns=model_names)\n",
    "\n",
    "\n",
    "for learner in model_names:\n",
    "    for n,dates in enumerate(date_ranges[:5]):\n",
    "        start_training, end_training, prediction_date = dates[0], dates[1],dates[2]\n",
    "        fit_X = X[start_training:end_training]\n",
    "        fit_y = y[start_training:end_training]\n",
    "        pred_X = X[prediction_date:prediction_date]\n",
    "        \n",
    "        learner.fit(fit_X, fit_y)\n",
    "        preds = learner.predict(pred_X)\n",
    "        sweep_results.loc[prediction_date,lerner] = learner.predict(pred_X)\n",
    "        \n",
    "\n",
    "sweep_results    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import *\n",
    "from sklearn.base import RegressorMixin\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "# Get all names in the module\n",
    "all_names = dir(linear_model)\n",
    "\n",
    "# Filter out regression models from the names\n",
    "regression_models = [\n",
    "    getattr(linear_model, name) for name in all_names if\n",
    "    isinstance(getattr(linear_model, name), type) and\n",
    "    issubclass(getattr(linear_model, name), RegressorMixin) and\n",
    "    len(inspect.signature(getattr(linear_model, name)).parameters) <= 2\n",
    "]\n",
    "\n",
    "# Print the list of regression model objects\n",
    "print(regression_models)\n",
    "\n",
    "sweep_results = pd.DataFrame(columns=[model.__name__ for model in regression_models])\n",
    "\n",
    "for learner in regression_models:\n",
    "    print(learner)\n",
    "    for n, dates in enumerate(date_ranges[:5]):\n",
    "        start_training, end_training, prediction_date = dates[0], dates[1], dates[2]\n",
    "        fit_X = X[start_training:end_training]\n",
    "        fit_y = y[start_training:end_training]\n",
    "        pred_X = X[prediction_date:prediction_date]\n",
    "\n",
    "        # Instantiate the learner\n",
    "        model_instance = learner()\n",
    "\n",
    "        # Fit the model\n",
    "        try:\n",
    "            model_instance.fit(fit_X, fit_y)\n",
    "\n",
    "            # Predict and store the result\n",
    "            preds = model_instance.predict(pred_X)\n",
    "            sweep_results.loc[prediction_date, model_instance.__class__.__name__] = preds\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipped model {learner.__name__} due to error: {e}\")\n",
    "\n",
    "sweep_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regression_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range to slice the dataset\n",
    "start_date = \"2023-01-03\"\n",
    "end_date = \"2023-01-07\"\n",
    "\n",
    "# Slice the dataset by the specified time range\n",
    "subset_ds = xr.sel(time=slice(start_date, end_date))\n",
    "\n",
    "# Calculate the cumulative sum along the time dimension\n",
    "cumulative_sum = subset_ds['data'].cumsum(dim='time')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "cumulative_sum.plot.line(marker='o')\n",
    "\n",
    "plt.title('Cumulative Sum of Data from {} to {}'.format(start_date, end_date))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Sum')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample time series dataset\n",
    "times = pd.date_range(\"2023-01-01\", \"2023-01-10\", freq=\"D\")\n",
    "data_values = np.random.rand(len(times))\n",
    "\n",
    "ds = xr.Dataset(\n",
    "    {\"data\": ((\"time\",), data_values)},\n",
    "    coords={\"time\": times},\n",
    ")\n",
    "\n",
    "# Define the time range to slice the dataset\n",
    "start_date = \"2023-01-03\"\n",
    "end_date = \"2023-01-07\"\n",
    "\n",
    "# Slice the dataset by the specified time range\n",
    "subset_ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "# Calculate the cumulative sum along the time dimension\n",
    "cumulative_sum = subset_ds['data'].cumsum(dim='time')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "cumulative_sum.plot.line(marker='o')\n",
    "\n",
    "plt.title('Cumulative Sum of Data from {} to {}'.format(start_date, end_date))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Sum')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_xr['predicted'].cumsum(dim='Date').plot.line(marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_xr['predicted'].cumsum(dim='Date').plot.line(marker='o', label='Cumulative Sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results_xr['rand_perf'].mean(dim='Tag').cumsum(dim='Date').plot.line(x='Date')\n",
    "Results_xr['rand_perf'].cumsum(dim='Date').plot.line(x='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Results_xr['predicted'].cumsum(dim='Date').to_dataframe().unstack()\n",
    "\n",
    "df.columns = df.columns.get_level_values(1)#['2023-11-28']\n",
    "df['2023-11-28'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=Results_xr.sel(Date=slice(\"2016-08-06\", \"2023-11-27\")).predicted.cumsum(axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=Results_xr.sel(Date=slice(\"2016-08-06\", \"2023-11-27\")).rand_perf.cumsum(axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=Results_xr.sel(Date=slice(\"2016-08-06\", \"2023-11-27\"),Tag='pca1reg').predicted#.mean(axis=1)\n",
    "# r=Results_xr.sel(Date=slice(\"2016-08-06\", \"2023-11-27\")).rand_perf\n",
    "# (f-r).mean()\n",
    "data = f.to_dataframe()['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_dataframe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regout.loc[regout['predicted'].dropna().index,'random'] = np.random.permutation(regout['predicted'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_means = np.mean(np.random.shuffle(data.copy()) for _ in range(100))\n",
    "permuted_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have an array 'data' that you want to permute\n",
    "# Replace this with your actual data\n",
    "#data = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Number of permutations\n",
    "num_permutations = 100\n",
    "\n",
    "# Generate 100 permuted arrays and calculate the mean for each\n",
    "permuted_means = np.mean([np.random.permutation(data).mean() for _ in range(num_permutations)])\n",
    "\n",
    "print(f\"Overall mean of permuted means: {permuted_means}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(data={'price':[100,110,121]})\n",
    "test_df['logret'] = log_returns(test_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Cumulative Log Returns'] = np.log1p(test_df['logret']).cumsum()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.DataFrame(data={'price':[100,110,121]})\n",
    "test_df['gpt Log Return'] = np.log(test_df['price'] / test_df['price'].shift(1))\n",
    "test_df['gpt cum'] = np.log1p(test_df['gpt Log Return']).cumsum()\n",
    "test_df['new_price'] = np.expm1(test_df['gpt cum']) * 100\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a DataFrame with a column 'price'\n",
    "test_df = pd.DataFrame(data={'price': [100, 110, 121]})\n",
    "\n",
    "# Calculate log returns\n",
    "test_df['gpt Log Return'] = np.log(test_df['price'] / test_df['price'].shift(1))\n",
    "\n",
    "# Calculate cumulative log returns\n",
    "test_df['gpt cum'] = np.log1p(test_df['gpt Log Return']).cumsum()\n",
    "\n",
    "# Calculate new prices based on cumulative log returns\n",
    "test_df['new_price'] = np.exp(test_df['gpt cum']) * 100\n",
    "\n",
    "print(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame with a 'price' column\n",
    "prices = [100, 110, 121, 132, 143]\n",
    "dates = pd.date_range('2023-01-01', periods=len(prices))\n",
    "df = pd.DataFrame(data={'price': prices}, index=dates)\n",
    "\n",
    "# Calculate arithmetic returns\n",
    "df['arith_return'] = df['price'].pct_change()\n",
    "\n",
    "# Calculate log returns\n",
    "df['log_return'] = np.log(df['price'] / df['price'].shift(1))\n",
    "df['log_ret_diff'] = np.log(df['price']) - np.log( df['price'].shift(1))\n",
    "\n",
    "# Accumulate returns over time\n",
    "df['cum_arith_return'] = (1 + df['arith_return']).cumprod() - 1\n",
    "df['cum_log_return'] = df['log_return'].cumsum()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(df.index, df['cum_arith_return'] * 100, label='Cumulative Arithmetic Return', marker='o')\n",
    "plt.plot(df.index, df['cum_log_return'] * 100, label='Cumulative Log Return', marker='o')\n",
    "\n",
    "plt.title('Comparison of Cumulative Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
