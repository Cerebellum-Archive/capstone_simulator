# 2023 Update to Capstone Project - Quantitative Trading Simulation Framework

<div align="center">

![Blue Water Macro Logo](docs/transparent-logo.png)

![Python](https://img.shields.io/badge/Python-3.8+-blue) ![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange) ![License](https://img.shields.io/badge/License-BWMEL_Educational-green) ![xarray](https://img.shields.io/badge/xarray-Multi--Dimensional-red) ![sklearn](https://img.shields.io/badge/sklearn-ML--Pipeline-yellowgreen)

**Keywords:** `quantitative trading` Â· `backtesting` Â· `Python` Â· `ETFs` Â· `financial engineering` Â· `machine learning` Â· `portfolio optimization` Â· `xarray` Â· `educational framework`

</div>

> **Updated quantitative trading simulation framework for backtesting strategies using Python, ideal for financial engineering education and research.**

A hands-on educational platform for **financial engineering students** to develop, backtest, and analyze trading strategies. Built with Python, xarray, and scikit-learn, this framework guides you through the **full quantitative research cycle**: data loading, feature engineering, model exploration, simulation, and reporting.

**Developed by Conrad Gann for Blue Water Macro Corp. Â© 2025**

## ðŸ“‹ Table of Contents
- [Why This Framework?](#why-this-framework)
- [Learning Objectives](#learning-objectives)
- [Quick Start](#quick-start)
- [Quick Wins - Copy & Paste Examples](#quick-wins---copy--paste-examples)
- [Repository Structure](#repository-structure)
- [Framework Components](#framework-components)
- [Model Complexity Scoring & Overfitting Detection](#-model-complexity-scoring--overfitting-detection)
- [Example Workflow](#example-workflow-full-research-cycle)
- [Hash-Based Future Testing Tutorial](#-tutorial-hash-based-future-testing)
- [Modern Hash Storage & Out-of-Sample Testing](#-modern-hash-storage--out-of-sample-testing)
- [Intelligent yfinance Caching System](#-intelligent-yfinance-caching-system)
- [Student Exercises & Capstone Ideas](#student-exercises--capstone-ideas)
- [Performance Expectations](#performance-expectations)
- [Educational Resources](#educational-resources)
- [Getting Help](#getting-help)

## Why This Framework?

- **Educational Focus**: Step-by-step tutorials teach core concepts like time-series cross-validation, multi-dimensional data handling with xarray, and risk-adjusted performance metrics
- **Full Research Cycle**: From raw data inputs to publication-quality reportsâ€”learn how quants at hedge funds structure their workflow
- **xarray for Finance**: Native use of xarray for standardized, multi-dimensional reporting (e.g., results across time/assets/strategies)â€”a skill increasingly valued in quant roles
- **Real-World Strategies**: Simulate single-asset (e.g., SPY) and multi-asset (e.g., SPY/QQQ/IWM) predictions with position sizing, leverage, and portfolio optimization
- **Capstone-Ready**: Ideal for financial engineering projectsâ€”includes exercises, extensions, and resources from QuantNet/CFA
- **Enterprise-Grade Analytics**: Leveraging Blue Water Macro's institutional insights and data science expertise

### Learning Objectives

By completing this framework, students will:

1. **Master time-series simulation** to avoid common pitfalls like look-ahead bias
2. **Use xarray for efficient data organization** and visualization in finance
3. **Build ML pipelines** for feature preprocessing and learner exploration
4. **Generate professional graphs and reports** for strategy evaluation
5. **Understand portfolio construction** and risk management principles
6. **Apply quantitative methods** used in institutional settings

## Quick Start

### Option 1: Install as Python Package (Recommended)
```bash
# Install directly from GitHub
pip install git+https://github.com/Cerebellum-Archive/capstone_simulator.git

# Or install in development mode after cloning
git clone https://github.com/Cerebellum-Archive/capstone_simulator.git
cd capstone_simulator
pip install -e .

# Launch tutorials
jupyter notebook notebooks/01_single_target_tutorial.ipynb
```

### Option 2: Traditional Setup
```bash
# 1. Clone the repository
git clone https://github.com/Cerebellum-Archive/capstone_simulator.git
cd capstone_simulator

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Launch Jupyter and start learning
jupyter notebook notebooks/01_single_target_tutorial.ipynb

# Optional: Generate offline PDF tutorial
python scripts/generate_pdf_simple.py
```

### Using the Package in Your Code
```python
# After installation, import anywhere in your Python environment
import capstone_simulator as cs

# Access key functions directly
from capstone_simulator import (
    log_returns, 
    EWMTransformer, 
    create_results_xarray,
    calculate_performance_metrics,
    get_complexity_score,
    calculate_complexity_adjusted_metrics
)

# Run simulations programmatically
data = cs.utils_simulate.download_etf_data(['SPY', 'QQQ'])
results = cs.single_target_simulator.run_simulation(data)
```

## ðŸš€ Quick Wins - Copy & Paste Examples

### Add a New ETF to Your Strategy
```python
# In notebooks/01_single_target_tutorial.ipynb or src/single_target_simulator.py
FEATURE_ETFS = ['XLK', 'XLF', 'XLV', 'XLY', 'XLP', 'XLE', 'XLI', 'XLB', 'XLU', 'VTI']  # Added VTI
```

### Switch to Different Target Assets
```python
# For single-target simulation
TARGET_ETF = 'QQQ'  # Instead of 'SPY'

# For multi-target portfolio
TARGET_ETFS = ['SPY', 'QQQ', 'IWM', 'EFA']  # Added international exposure
```

### Quick Model Comparison
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge

# Compare multiple models in one run
models = {
    'ridge': Ridge(alpha=1.0),
    'rf': RandomForestRegressor(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    results[name] = simulate_strategy(features, targets, model=model)
```

### Generate Performance Report in 3 Lines
```python
from src.utils_simulate import create_results_xarray, calculate_performance_metrics

results_xr = create_results_xarray(your_results)
metrics = calculate_performance_metrics(results_xr)
print(f"Sharpe Ratio: {metrics['sharpe']:.2f}")
```

### Check Model Complexity & Overfitting Risk
```python
from capstone_simulator import get_complexity_score, calculate_complexity_adjusted_metrics

# Get complexity score for any sklearn model
complexity = get_complexity_score(your_model)
print(f"Model complexity score: {complexity:.2f}")

# Get complexity-adjusted performance metrics
adj_metrics = calculate_complexity_adjusted_metrics(strategy_returns, complexity)
print(f"Raw Sharpe: {adj_metrics['sharpe_ratio']:.2f}")
print(f"Complexity-Adjusted Sharpe: {adj_metrics['complexity_adjusted_sharpe']:.2f}")
print(f"Overfitting Risk: {1 - adj_metrics['overfitting_penalty']:.1%}")
```

### Avoid yfinance API Limits with Smart Caching
```python
from src.multi_target_simulator import download_etf_data_with_cache, list_yfinance_cache

# Download with automatic caching (eliminates "ETF not found" warnings)
data = download_etf_data_with_cache(['SPY', 'QQQ', 'IWM'], start_date='2020-01-01')
print(f"Downloaded {data.shape[0]} days of data for {data.shape[1]} ETFs")

# View your cached datasets
list_yfinance_cache()  # Shows tickers, dates, file sizes, and ages

# All simulations automatically use cached data - no more API limits!
```

## Repository Structure

```
quant_trading_simulator/
â”œâ”€â”€ src/                    # Core production code (modular for extensions)
â”‚   â”œâ”€â”€ utils_simulate.py   # Utilities (expanded with xarray transformers)
â”‚   â”œâ”€â”€ single_target_simulator.py  # Basic SPY prediction simulator
â”‚   â””â”€â”€ multi_target_simulator.py   # Advanced multi-asset simulator
â”œâ”€â”€ notebooks/              # Interactive educational tutorials
â”‚   â”œâ”€â”€ 01_single_target_tutorial.ipynb    # Basics: Single-asset simulation
â”‚   â”œâ”€â”€ 02_multi_target_tutorial.ipynb     # Advanced: Multi-asset strategies
â”‚   â””â”€â”€ 03_full_research_cycle.ipynb       # End-to-end project demo
â”œâ”€â”€ docs/                   # Documentation and guides
â”‚   â”œâ”€â”€ ERM3_2e_Data_Dictionary.md         # Blue Water Macro data schema
â”‚   â””â”€â”€ README.md                          # This guide
â”œâ”€â”€ data/                   # Sample datasets (user-provided)
â”œâ”€â”€ reports/                # Auto-generated outputs (plots, CSVs)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ .gitignore             # Standard project ignores
```

## Framework Components

### Core Modules (`src/`)
- **`utils_simulate.py`**: Essential utilities for data preparation, returns calculation, and xarray operations
- **`single_target_simulator.py`**: Educational single-asset (SPY) prediction framework
- **`multi_target_simulator.py`**: Production-ready multi-asset prediction and portfolio construction

### Educational Materials (`notebooks/`)
- **Tutorial 1**: Single-target simulation walkthrough with sector ETFs
- **Tutorial 2**: Multi-asset portfolio strategies with advanced position sizing
- **Tutorial 3**: Complete research cycle demonstration with exercises

### Documentation (`docs/`)
- **ðŸ“– Complete PDF Tutorial**: Professional offline guide (`Blue_Water_Macro_Tutorial.pdf`)
- **ðŸŒ Interactive HTML Tutorial**: Browser-based tutorial with print-to-PDF capability
- **Data Dictionary**: Blue Water Macro's ERM3 model documentation
- **Implementation Guides**: Best practices and advanced techniques

## ðŸ§  Model Complexity Scoring & Overfitting Detection

The framework includes sophisticated **model complexity analysis** to help identify and mitigate overfitting risksâ€”a critical concern in quantitative finance where complex models may capture noise rather than genuine market signals.

### Key Features

- **ðŸŽ¯ Automated Complexity Scoring**: Each model receives a complexity score based on its architecture and hyperparameters
- **ðŸ“Š Complexity-Adjusted Metrics**: Performance metrics adjusted for model complexity to identify truly robust strategies
- **âš–ï¸ Overfitting Risk Assessment**: Early warning system for models that may not generalize well
- **ðŸ” Meta-Analysis Tools**: Compare strategies with similar complexity scores for fair evaluation

### Complexity Score Examples

```python
from capstone_simulator import get_complexity_score, calculate_complexity_adjusted_metrics

# Simple models (lower scores = less overfitting risk)
ridge_model = Ridge(alpha=1.0)          # Score: ~0.5
ols_model = LinearRegression()          # Score: 1.0 (baseline)

# Complex models (higher scores = higher overfitting risk) 
rf_model = RandomForestRegressor()      # Score: ~11.0
xgb_model = XGBRegressor()             # Score: ~2.3

# Hyperparameter search amplifies complexity
grid_search = GridSearchCV(rf_model, param_grid) # Score: ~50+ (depending on search space)

# Calculate complexity-adjusted performance metrics
returns = your_strategy_returns
complexity_metrics = calculate_complexity_adjusted_metrics(returns, complexity_score)

print(f"Raw Sharpe Ratio: {metrics['sharpe_ratio']:.2f}")
print(f"Complexity-Adjusted Sharpe: {complexity_metrics['complexity_adjusted_sharpe']:.2f}")
print(f"Overfitting Risk Score: {complexity_metrics['overfitting_penalty']:.2f}")
```

### Educational Value

Understanding model complexity teaches crucial quantitative finance concepts:

- **ðŸŽ“ Bias-Variance Trade-off**: Balance between model flexibility and generalization
- **ðŸ”¬ Out-of-Sample Robustness**: Why simpler models often perform better in production
- **ðŸ“ˆ Production Deployment**: How complexity affects real-world trading performance
- **âš¡ Risk Management**: Early detection of potentially unreliable signals

The complexity scoring system integrates seamlessly with the framework's metadata system, automatically calculating scores based on simulation configurations stored in the reproducible hash system.

## Example Workflow (Full Research Cycle)

```python
# 1. INPUTS: Load ETF data via yfinance
import yfinance as yf
data = yf.download(['SPY', 'XLK', 'XLF'], start='2020-01-01')

# 2. PREPROCESSING: Feature engineering with xarray
from src.utils_simulate import EWMTransformer, create_results_xarray
transformer = EWMTransformer(halflife=5)
features_smooth = transformer.fit_transform(features)

# 3. EXPLORATION: Analyze feature importance
from src.utils_simulate import p_by_year
feat_analysis = p_by_year(X_features, y_target)

# 4. SIMULATION: Walk-forward backtesting
results = simulate_strategy(features_smooth, targets, model='ridge')

# 5. REPORTING: Generate publication-quality outputs
results_xr = create_results_xarray(results)
results_xr.plot.line(x='time', col='strategy', col_wrap=2)
```

See `notebooks/03_full_research_cycle.ipynb` for a complete implementation.

## ðŸ• Tutorial: Hash-Based Future Testing

This tutorial demonstrates the **killer feature** of the framework: running your exact historical strategy on future data to test true out-of-sample performance.

### Step-by-Step: Today's Strategy, Tomorrow's Data

#### Phase 1: Run Initial Strategy (January 2025)
```python
# Run your strategy on historical data (2020-2024)
from src.multi_target_simulator import run_simulation

results = run_simulation(
    target_etfs=['SPY', 'QQQ', 'IWM'],
    feature_etfs=['XLK', 'XLF', 'XLV', 'XLY', 'XLP'],
    model='RandomForestRegressor',
    params={'n_estimators': 100, 'max_depth': 5},
    training_window_size=400,
    position_sizer='confidence_weighted',
    start_date='2020-01-01',
    end_date='2024-12-31'
)

# Note the hash for future use
strategy_hash = results['simulation_hash']
print(f"Strategy Hash: {strategy_hash}")
print(f"Trained on: 2020-2024")
print(f"In-Sample Sharpe: {results['sharpe_ratio']:.2f}")

# Hash stored automatically: cache/simulation_{hash}_multi_target.zarr
```

#### Phase 2: Wait for New Data (June 2025)
```python
# 6 months later... new market data is available
# Your strategy parameters are perfectly preserved in the hash

from src.multi_target_simulator import reconstruct_pipeline_from_metadata, run_reconstructed_pipeline

# Reconstruct EXACT same pipeline from hash
pipeline_config = reconstruct_pipeline_from_metadata(
    simulation_hash=strategy_hash,  # From January
    tag='multi_target'
)

print("Reconstructed Strategy:")
print(f"Model: {pipeline_config['model']}")
print(f"Parameters: {pipeline_config['params']}")
print(f"Training Window: {pipeline_config['training_window_size']}")
print(f"Original Born Date: {pipeline_config['born_on_date']}")
```

#### Phase 3: Test on Truly Unseen Data
```python
# Download completely new data (2025 Q1-Q2)
from src.utils_simulate import download_etf_data, prepare_features_targets

new_data = download_etf_data(
    tickers=['SPY', 'QQQ', 'IWM', 'XLK', 'XLF', 'XLV', 'XLY', 'XLP'],
    start_date='2025-01-01',  # Data the model has NEVER seen
    end_date='2025-06-30'
)

X_new, y_new = prepare_features_targets(new_data, pipeline_config)

# Run EXACT same strategy on new data
oos_results = run_reconstructed_pipeline(pipeline_config, X_new, y_new)

print("\n=== OUT-OF-SAMPLE RESULTS ===")
print(f"Original Sharpe (2020-2024): {results['sharpe_ratio']:.2f}")
print(f"Out-of-Sample Sharpe (2025): {oos_results['sharpe_ratio']:.2f}")
print(f"Strategy Robustness: {oos_results['sharpe_ratio']/results['sharpe_ratio']:.1%}")

# If robustness > 80%, strategy shows genuine alpha
# If robustness < 50%, likely overfitted to historical data
```

#### Phase 4: Generate Comparative Report
```python
# Create side-by-side performance analysis
from src.plotting_utils import create_performance_tearsheet

# Generate report showing in-sample vs out-of-sample
create_performance_tearsheet(
    in_sample_results=results,
    out_of_sample_results=oos_results,
    strategy_hash=strategy_hash,
    title="Strategy Robustness Analysis: Jan 2025 vs Jun 2025"
)

# PDF report includes:
# - Vertical line at born_on_date (when strategy was created)
# - Cumulative returns showing strategy decay/persistence
# - Risk metrics comparison (Sharpe, Calmar, Max Drawdown)
# - Statistical significance tests
```

### Real-World Applications

**ðŸŽ“ Academic Research**: 
```python
# Thesis defense: "I trained this strategy in January, here's how it performed 
# on unseen March data with no modifications"
```

**ðŸ›ï¸ Institutional Validation**: 
```python
# Hedge fund presentation: "Our model shows 85% robustness over 6-month 
# out-of-sample period, indicating genuine alpha discovery"
```

**ðŸ“Š Strategy Monitoring**:
```python
# Monthly strategy review: Compare current performance against 
# original hash to detect strategy decay
```

### Advanced Hash Management

```python
# List all historical strategies
from src.multi_target_simulator import list_cached_simulations

strategies = list_cached_simulations()
for hash_id, metadata in strategies.items():
    print(f"Hash: {hash_id}")
    print(f"Created: {metadata['born_on_date']}")
    print(f"Model: {metadata['model']}")
    print(f"Performance: {metadata.get('sharpe_ratio', 'N/A')}")
    print("---")

# Clean up old experiments
cleanup_old_hashes(older_than_days=90)
```

This workflow represents **institutional-grade strategy validation** - the same process used by quantitative hedge funds to distinguish genuine alpha from statistical noise.

## ðŸ“– Offline PDF Tutorial

For comprehensive offline study, we provide multiple options:

### Option 1: Direct PDF Generation (Recommended)
```bash
# Generate professional PDF directly
python scripts/generate_pdf_simple.py
```

### Option 2: HTML + Browser Print
```bash
# Generate interactive HTML tutorial
python scripts/generate_html_tutorial.py

# Then use your browser's "Print to PDF" function (Ctrl+P or Cmd+P)
```

**What's included:**
- âœ… **Complete Framework Guide**: All concepts from basic to advanced
- âœ… **Professional Styling**: Blue Water Macro corporate design
- âœ… **Code Examples**: Syntax-highlighted Python with copy buttons  
- âœ… **Career Guidance**: Pathways to quantitative finance roles
- âœ… **Mathematical Foundations**: Formulas and derivations
- âœ… **Offline Access**: No internet required once generated

**Perfect for:**
- ðŸ“š **Student Study Materials**: Comprehensive reference guide
- ðŸ’¼ **Career Portfolios**: Demonstrate quantitative expertise
- ðŸŽ“ **Academic Courses**: Ready-to-use educational content
- ðŸ“‹ **Quick Reference**: Formulas and code templates

## Student Exercises & Capstone Ideas

### Beginner Exercises
1. **Volatility Targeting**: Implement position sizing based on realized volatility
2. **Feature Engineering**: Add momentum indicators and technical analysis features
3. **Model Comparison**: Compare Ridge, Random Forest, and Linear models using xarray coordinates

### Advanced Capstone Projects
1. **Regime Detection**: Implement bull/bear market switching using hidden Markov models
2. **Transaction Costs**: Add realistic trading costs and slippage to backtests
3. **Risk Management**: Implement VaR-based position limits and drawdown controls
4. **Alternative Data**: Integrate sentiment analysis or economic indicators
5. **Portfolio Optimization**: Apply Modern Portfolio Theory for asset allocation

## ðŸ’¾ Modern Hash Storage & Out-of-Sample Testing

The framework uses **xarray.zarr format** for intelligent caching and complete reproducibility. Every simulation generates a unique hash based on all parameters (data sources, model configuration, training windows, etc.) and stores both results and complete metadata for future reconstruction.

### Zarr-Based Hash Storage System

```python
# Automatic hash generation and storage
from src.multi_target_simulator import run_simulation

# Run initial simulation (automatically cached)
results = run_simulation(
    target_etfs=['SPY', 'QQQ'], 
    model='RandomForestRegressor',
    params={'n_estimators': 100},
    training_window_size=400
)

# Hash automatically generated: simulation_a1b2c3d4_multi_target.zarr
print(f"Simulation hash: {results['simulation_hash']}")
```

### True Out-of-Sample Testing

The revolutionary feature is **pipeline reconstruction from hash files** - allowing you to test how your **exact same strategy** performs on completely new, unseen data:

```python
from src.multi_target_simulator import reconstruct_pipeline_from_metadata, run_reconstructed_pipeline

# 1. Reconstruct the EXACT pipeline from hash
pipeline_config = reconstruct_pipeline_from_metadata(
    simulation_hash='a1b2c3d4',  # From your original run
    tag='multi_target'
)

# 2. Test on completely new data (e.g., 2025 data when original was 2020-2024)
new_data = download_etf_data(['SPY', 'QQQ'], start_date='2025-01-01')
X_new, y_new = prepare_features_targets(new_data)

# 3. Run EXACT same strategy on new data
oos_results = run_reconstructed_pipeline(pipeline_config, X_new, y_new)

# 4. Compare performance: original vs out-of-sample
print(f"Original Sharpe: {original_sharpe:.2f}")
print(f"Out-of-Sample Sharpe: {oos_sharpe:.2f}")
print(f"Strategy robustness: {oos_sharpe/original_sharpe:.1%}")
```

### Complete Metadata Preservation

Each zarr file contains **everything needed** for perfect reproduction:

```python
# Load any historical simulation
results_df, metadata = load_simulation_results('a1b2c3d4', 'multi_target')

print(metadata['born_on_date'])        # '2025-01-15T10:30:45'
print(metadata['target_etfs'])         # ['SPY', 'QQQ']
print(metadata['model_pipeline'])      # 'Pipeline([('ewm', EWMTransformer(halflife=5)), ...]'
print(metadata['training_window'])     # 400
print(metadata['data_fingerprint'])    # Hash of exact data used
print(metadata['framework_version'])   # '2.1.0'
```

### Easy Born-on-Date Access

The `born_on_date` is now stored as an **xarray coordinate** for lightning-fast access:

```python
# Quick access without loading full metadata
from src.multi_target_simulator import get_born_on_date_from_zarr
born_date = get_born_on_date_from_zarr('a1b2c3d4', 'multi_target')
print(f"Strategy created: {born_date}")  # '2025-01-15T10:30:45'

# Direct xarray coordinate access
import xarray as xr
ds = xr.open_zarr('cache/simulation_a1b2c3d4_multi_target.zarr')
print(f"Born on: {ds.coords['born_on_date'].values}")  # Ultra-fast!

# Perfect for filtering or plotting
strategies_by_date = {
    hash_id: str(ds.coords['born_on_date'].values) 
    for hash_id in strategy_hashes
}
```

### Institutional-Grade Benefits

This approach mirrors **hedge fund practices** where strategies must prove robustness across time periods:

- **ðŸŽ¯ Overfitting Detection**: Compare in-sample vs out-of-sample performance
- **ðŸ“Š Strategy Decay Analysis**: Track how performance changes over time
- **ðŸ”„ Reproducible Research**: Share exact configurations with colleagues
- **ðŸ›ï¸ Regulatory Compliance**: Complete audit trail for institutional use
- **ðŸ“ˆ Production Deployment**: Test strategies before live trading

## ðŸŒ Intelligent yfinance Caching System

The framework includes a comprehensive **yfinance caching system** using zarr format to eliminate API rate limiting issues during development and research. This system automatically caches all ETF downloads, dramatically improving workflow efficiency.

### Automatic Cache Management

```python
from src.multi_target_simulator import download_etf_data_with_cache

# Automatic caching with configurable age limits
data = download_etf_data_with_cache(
    tickers=['SPY', 'QQQ', 'IWM'],
    start_date='2020-01-01',
    max_age_hours=24  # Refresh cache after 24 hours
)

# First run: Downloads from yfinance and caches
# Subsequent runs: Uses cached data (lightning fast!)
```

### Cache Benefits & Features

**ðŸš€ Performance Improvements:**
- **Instant Re-runs**: Cached data loads in milliseconds vs minutes for downloads
- **API Limit Protection**: Eliminates "ETF not found, using zeros" warnings
- **Offline Development**: Work without internet once data is cached
- **Batch Processing**: Run multiple simulations without API restrictions

**ðŸ’¾ Smart Storage:**
```python
# Cache files stored in: cache/yfinance_data/
# Example: yf_SPY_QQQ_IWM_2020-01-01_2025-07-29.zarr

# Cache management utilities
from src.multi_target_simulator import list_yfinance_cache, clean_yfinance_cache

# View all cached datasets
list_yfinance_cache()
# Output shows: tickers, date ranges, file sizes, ages

# Clean old cache files (saves disk space)
clean_yfinance_cache(max_age_days=7)  # Remove files older than 7 days
```

**ðŸ”§ Technical Architecture:**
- **Multi-Ticker Support**: Handles both single and multi-asset downloads seamlessly
- **Column Structure Preservation**: Maintains proper yfinance MultiIndex format
- **Metadata Tracking**: Stores download timestamps, ticker lists, date ranges
- **Error Recovery**: Graceful handling of individual ticker failures
- **Memory Efficient**: Zarr format with compression for large datasets

### Integration with Simulations

Both simulation engines automatically use the caching system:

```python
# Single-target simulator with caching
from src.single_target_simulator import load_and_prepare_data
X, y = load_and_prepare_data(['SPY', 'QQQ'], target_etf='SPY', start_date='2020-01-01')
# Uses cached data if available, downloads if needed

# Multi-target simulator with caching  
from src.multi_target_simulator import Simulate_MultiTarget
results = Simulate_MultiTarget(
    target_etfs=['SPY', 'QQQ', 'IWM'],
    feature_etfs=['XLK', 'XLF', 'XLV'],
    start_date='2020-01-01'
)
# All ETF downloads automatically cached
```

### Real-World Workflow Benefits

**ðŸŽ“ Educational Use Cases:**
```python
# Students can experiment freely without API limits
for window_size in [200, 400, 600]:
    for model in ['ridge', 'rf', 'xgb']:
        results = run_simulation(window_size=window_size, model=model)
        # Each run reuses cached data - no download delays!
```

**ðŸ›ï¸ Research Applications:**
```python
# Researchers can iterate rapidly on strategy development
# Cache eliminates the "download bottleneck" in quantitative research
results_list = []
for strategy_config in strategy_grid:
    results = backtest_strategy(strategy_config)  # Instant data access
    results_list.append(results)
```

**âš¡ Production Benefits:**
- **Consistent Data**: Same cached dataset ensures reproducible results
- **Fast Prototyping**: Rapid strategy iteration without download delays  
- **Offline Capability**: Continue working during network outages
- **Cost Efficiency**: Reduces API usage for institutional subscriptions

## ðŸ”¬ Benefits of Enhanced Reproducibility

The expanded metadata hashing in the Capstone Simulator Framework ensures full reproducibility by storing all key parameters, such as data sources, training configurations, model pipelines, and simulation details. This structured approach, combined with the framework's comprehensive caching and metadata tracking, delivers significant benefits for quantitative trading research, particularly in addressing overfitting risks and fostering reliable alpha discovery.

### Key Benefits

#### **Mitigating Overfitting through a Reference Base**
By storing complete experiment configurations (e.g., ETF symbols, training windows, hyperparameters), the framework creates a robust reference base of all simulations. This allows researchers to systematically compare strategies across runs, identifying patterns that may indicate overfitting (e.g., strategies that perform well only on specific parameter sets). A comprehensive record of experiments helps distinguish true alphaâ€”signals with consistent predictive powerâ€”from spurious results driven by data-specific noise or over-optimized parameters.

#### **Enhanced Research Rigor**
Full reproducibility ensures every simulation can be recreated exactly, enabling researchers to validate results and build confidence in findings. The metadata (e.g., `random_seed=42`, `window_size=400`) acts as a blueprint, allowing users to revisit past experiments and assess their stability across different time periods or market conditions, reducing the risk of overfitting to a single dataset or period.

#### **Audit Trail for Transparency**
The detailed metadata structure supports regulatory compliance and academic scrutiny by providing a complete audit trail. This transparency is critical in financial research, where documenting the exact data (e.g., `start_date`, `feature_columns`) and model configurations (e.g., `pipe_steps`, `param_grid`) helps verify that performance metrics reflect genuine predictive power rather than overfitting artifacts.

#### **Facilitating Collaboration and Knowledge Sharing**
Researchers and students can share exact simulation setups via metadata, fostering collaboration without ambiguity. This shared reference base allows teams to replicate experiments, refine strategies, and collectively evaluate whether discovered signals are robust or likely overfitted to historical data, accelerating the identification of true alpha.

#### **Version Control and Backward Compatibility**
Tracking `framework_version` and `python_version` ensures experiments remain reproducible even as the codebase evolves. This preserves the integrity of historical results, allowing users to compare new strategies against a baseline of past runs to assess whether improvements are genuine or artifacts of overfitting to new data or configurations.

#### **Educational Value for Understanding Alpha Discovery**
For students, the metadata-driven approach highlights the importance of reproducibility in financial modeling. By cataloging all experiments, it teaches the discipline of systematic testing, helping learners recognize overfitting pitfalls (e.g., cherry-picking parameters) and focus on strategies with repeatable performance, which are more likely to yield true alpha in live trading.

### Linking to Alpha Discovery

The reference base created by storing all experiments is a powerful tool for evaluating the likelihood of discovering true alpha. By maintaining a comprehensive record of simulationsâ€”including data fingerprints, model parameters, and performance metricsâ€”researchers can analyze the consistency of strategies across diverse conditions. Strategies that perform well across varied training windows, asset sets, or market regimes are less likely to be overfitted and more likely to capture genuine market inefficiencies. This systematic approach enables users to prioritize robust signals, filter out noise-driven results, and build confidence in strategies that hold up under scrutiny, ultimately increasing the probability of uncovering true alpha.

## Performance Expectations

Based on historical backtests using Blue Water Macro's methodology (2015-2024):

| Strategy Type | Typical Sharpe | Max Drawdown | Best Use Case |
|---------------|----------------|--------------|---------------|
| **Equal Weight** | 0.8 - 1.2 | 15-25% | Conservative, diversified |
| **Confidence Weighted** | 1.0 - 1.8 | 20-35% | Moderate risk, adaptive |
| **Long-Short** | 1.2 - 2.5 | 10-20% | Market-neutral strategies |

## Educational Resources

### Academic References
- **QuantNet Forums**: [quantnet.com](https://quantnet.com) - Premier quantitative finance community
- **CFA Institute**: [cfainstitute.org](https://cfainstitute.org) - Professional certification and readings
- **xarray Documentation**: [xarray.pydata.org](https://xarray.pydata.org/en/stable/examples/finance.html) - Financial data handling
- **Risk Models**: [riskmodels.net](https://riskmodels.net) - Institutional beta factors, grade gross and residual returns

### Professional Development
- **Blue Water Macro**: Enterprise-grade data and analytics for institutional research
- **Industry Applications**: Portfolio management, risk analytics, algorithmic trading
- **Career Paths**: Quantitative researcher, portfolio manager, risk analyst

## ðŸ§ª Running Tests

The framework includes comprehensive unit tests to ensure reliability and correctness of quantitative trading simulations. Tests cover core utilities, simulation engines, caching, and educational functions.

### Quick Test Commands

```bash
# Run all tests
pytest

# Run tests with coverage report
pytest --cov=src --cov-report=html

# Run specific test modules
pytest tests/test_utils_simulate.py
pytest tests/test_single_target_simulator.py
pytest tests/test_multi_target_simulator.py

# Run only fast tests (exclude slow integration tests)
pytest -m "not slow"

# Run tests with verbose output
pytest -v

# Run tests in parallel (if you have pytest-xdist installed)
pytest -n auto
```

### Test Categories

- **Unit Tests**: Test individual functions and classes in isolation
- **Integration Tests**: Test complete workflows and component interactions  
- **Performance Tests**: Verify reasonable execution times for large datasets
- **Error Handling Tests**: Ensure graceful handling of edge cases and invalid inputs

### Educational Value of Testing

The test suite serves as:
- **Living Documentation**: Examples of how to use each function correctly
- **Quality Assurance**: Confidence that simulations produce reliable results
- **Regression Prevention**: Ensures changes don't break existing functionality
- **Learning Tool**: Students can study tests to understand expected behaviors

## ðŸ”§ Troubleshooting

### Common Installation Issues
```bash
# If you get permission errors on macOS/Linux
pip install --user -r requirements.txt

# If Jupyter won't start
pip install --upgrade jupyter notebook

# If yfinance fails to download data
pip install --upgrade yfinance requests
```

### Runtime Issues
- **"Module not found" errors**: Ensure you're in the correct directory and virtual environment is activated
- **Data download failures**: Check your internet connection and try running the download cell again
- **Memory errors with large datasets**: Reduce the date range or number of ETFs in your configuration
- **Plot not displaying**: Run `%matplotlib inline` in your Jupyter notebook

### Performance Tips
- **Faster backtests**: Reduce `TRAINING_WINDOW_SIZE` or increase `REBALANCE_FREQ` in configuration
- **Better predictions**: Experiment with different `EWM_HALFLIFE` values (try 3, 5, 10, 20 days)
- **Cleaner results**: Use `results_xr.sel(strategy='equal_weight')` to focus on one strategy

## Getting Help

### Technical Support
- **GitHub Issues**: Report bugs and request features
- **Documentation**: Comprehensive guides in `docs/` folder
- **Code Comments**: Detailed inline explanations throughout codebase

### Academic Guidance
- **Office Hours**: Contact [Capstone@BlueWaterMacro.com] for capstone project guidance
- **Collaboration**: Connect with other students on QuantNet forums
- **Industry Mentorship**: Blue Water Macro internship and career opportunities

## Disclaimer

This educational framework is provided for learning purposes. Past performance does not guarantee future results. Always conduct thorough due diligence before making investment decisions. Blue Water Macro Corp. provides no warranty regarding the accuracy or completeness of this software.

## License

This project is licensed under the **Blue Water Macro Educational License (BWMEL)** - see the [LICENSE](LICENSE) file for complete terms.

### ðŸ“š **Educational Use**
- âœ… **Free for students and educators** at accredited institutions
- âœ… **Research and academic publication** permitted with attribution
- âœ… **Capstone projects and dissertations** encouraged
- âœ… **Modifications for educational purposes** allowed

### ðŸ¢ **Commercial Use**
- ðŸ“§ **Commercial licensing available** - contact licensing@bluewatermacro.com
- ðŸ¤ **Enterprise partnerships welcome** for institutional adoption
- ðŸ’¼ **Career opportunities** for exceptional student contributors

### âš–ï¸ **Key Requirements**
- **Attribution**: Must credit "Blue Water Macro Quantitative Trading Framework"
- **Educational Focus**: Commercial use requires separate license
- **Share-Alike**: Educational improvements should benefit the community
- **Financial Disclaimer**: For educational purposes only - not investment advice

---

**Build your quantitative finance career with enterprise-grade tools and institutional insights.**

*Leveraging Blue Water Macro's ERM3 model for comprehensive market analysis and strategy development.*